{% extends "base.html" %}

{% block content %}
<!-- Page Header -->
<section class="mb-12">
    <h1 class="text-3xl font-bold text-gray-900 mb-2">Methodology</h1>
    <p class="text-gray-600 text-lg">
        Comprehensive technical details of the LegalGPT graph-augmented legal prediction system
    </p>
    <div class="mt-4 flex flex-wrap gap-2">
        <span class="px-3 py-1 bg-blue-100 text-blue-700 rounded-full text-sm">GraphSAGE</span>
        <span class="px-3 py-1 bg-purple-100 text-purple-700 rounded-full text-sm">QLoRA</span>
        <span class="px-3 py-1 bg-green-100 text-green-700 rounded-full text-sm">RAG</span>
        <span class="px-3 py-1 bg-yellow-100 text-yellow-700 rounded-full text-sm">Mistral-7B</span>
    </div>
</section>

<!-- Table of Contents -->
<section class="card p-6 mb-8 bg-gray-50">
    <h2 class="font-bold text-lg mb-4">Table of Contents</h2>
    <div class="grid md:grid-cols-3 gap-4 text-sm">
        <div>
            <div class="font-medium text-gray-700 mb-2">Architecture</div>
            <ul class="space-y-1 text-gray-600">
                <li><a href="#system-overview" class="hover:text-blue-600">1. System Overview</a></li>
                <li><a href="#task-formulation" class="hover:text-blue-600">2. Task Formulation</a></li>
                <li><a href="#graphsage" class="hover:text-blue-600">3. GraphSAGE Embeddings</a></li>
            </ul>
        </div>
        <div>
            <div class="font-medium text-gray-700 mb-2">Training</div>
            <ul class="space-y-1 text-gray-600">
                <li><a href="#qlora" class="hover:text-blue-600">4. QLoRA Fine-tuning</a></li>
                <li><a href="#loss-functions" class="hover:text-blue-600">5. Loss Functions</a></li>
                <li><a href="#negative-sampling" class="hover:text-blue-600">6. Negative Sampling</a></li>
            </ul>
        </div>
        <div>
            <div class="font-medium text-gray-700 mb-2">Inference</div>
            <ul class="space-y-1 text-gray-600">
                <li><a href="#hybrid-retrieval" class="hover:text-blue-600">7. Hybrid Retrieval</a></li>
                <li><a href="#embedding-fusion" class="hover:text-blue-600">8. Embedding Fusion</a></li>
                <li><a href="#prompt-template" class="hover:text-blue-600">9. Prompt Engineering</a></li>
            </ul>
        </div>
        <div>
            <div class="font-medium text-gray-700 mb-2">Analysis</div>
            <ul class="space-y-1 text-gray-600">
                <li><a href="#attention-analysis" class="hover:text-blue-600">10. Attention Analysis</a></li>
                <li><a href="#calibration" class="hover:text-blue-600">11. Calibration</a></li>
                <li><a href="#statistical-tests" class="hover:text-blue-600">12. Statistical Tests</a></li>
            </ul>
        </div>
        <div>
            <div class="font-medium text-gray-700 mb-2">Implementation</div>
            <ul class="space-y-1 text-gray-600">
                <li><a href="#computational" class="hover:text-blue-600">13. Computational Requirements</a></li>
                <li><a href="#reproducibility" class="hover:text-blue-600">14. Reproducibility</a></li>
            </ul>
        </div>
    </div>
</section>

<!-- Section 1: System Overview -->
<section id="system-overview" class="card card-academic p-8 mb-8">
    <h2 class="section-header text-2xl font-bold">
        <span class="section-number">1</span>
        System Architecture Overview
    </h2>

    <p class="text-gray-600 mb-6">
        LegalGPT implements a three-stage pipeline combining graph neural networks for precedent retrieval with large language models for outcome prediction. The architecture is inspired by retrieval-augmented generation (RAG) frameworks (Lewis et al., 2020) but incorporates citation graph structure as a first-class signal.
    </p>

    <div class="bg-gray-100 p-6 rounded-lg mb-6 overflow-x-auto">
        <pre class="text-sm font-mono">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              LEGALGPT SYSTEM ARCHITECTURE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                                â”‚
â”‚  â”‚   INPUT CASE    â”‚                                                                â”‚
â”‚  â”‚  (text, meta)   â”‚                                                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                                â”‚
â”‚           â”‚                                                                          â”‚
â”‚           â–¼                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         STAGE 1: GRAPH RETRIEVAL                             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚  â”‚  â”‚ Citation Graphâ”‚â”€â”€â–¶â”‚  GraphSAGE    â”‚â”€â”€â–¶â”‚  Hybrid       â”‚                  â”‚   â”‚
â”‚  â”‚  â”‚ (Neo4j)       â”‚   â”‚  Encoder      â”‚   â”‚  Retriever    â”‚                  â”‚   â”‚
â”‚  â”‚  â”‚ 10K nodes     â”‚   â”‚  (2-layer)    â”‚   â”‚  (k=5)        â”‚                  â”‚   â”‚
â”‚  â”‚  â”‚ 150K edges    â”‚   â”‚  128-dim out  â”‚   â”‚  Î±=0.6        â”‚                  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                          â”‚
â”‚           â–¼  [Top-k precedents with relevance scores]                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         STAGE 2: CONTEXT ASSEMBLY                            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚  [INST] System prompt + Query case + Retrieved precedents [/INST]   â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Query case: ~5,000 tokens (truncated)                            â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Each precedent: ~3,000 tokens Ã— 5 = 15,000 tokens               â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Total context: ~20,000 tokens (within 32K limit)                 â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                          â”‚
â”‚           â–¼                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         STAGE 3: LLM PREDICTION                              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚  â”‚  â”‚ Mistral-7B    â”‚â”€â”€â–¶â”‚   QLoRA       â”‚â”€â”€â–¶â”‚ Classificationâ”‚                  â”‚   â”‚
â”‚  â”‚  â”‚ Instruct v0.3 â”‚   â”‚   Adapters    â”‚   â”‚    Head       â”‚                  â”‚   â”‚
â”‚  â”‚  â”‚ (frozen)      â”‚   â”‚   r=16, Î±=32  â”‚   â”‚  4096â†’2       â”‚                  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                          â”‚
â”‚           â–¼                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                                â”‚
â”‚  â”‚     OUTPUT      â”‚                                                                â”‚
â”‚  â”‚ P(petitioner)   â”‚                                                                â”‚
â”‚  â”‚ P(respondent)   â”‚                                                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                                â”‚
â”‚                                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        </pre>
    </div>

    <div class="grid md:grid-cols-3 gap-6">
        <div class="callout callout-info">
            <h4 class="font-medium mb-2">Stage 1: Graph Retrieval</h4>
            <p class="text-sm text-gray-600">GraphSAGE learns node embeddings that capture both semantic content and citation structure. Hybrid scoring combines embedding similarity with graph proximity.</p>
        </div>
        <div class="callout callout-success">
            <h4 class="font-medium mb-2">Stage 2: Context Assembly</h4>
            <p class="text-sm text-gray-600">Retrieved precedents are formatted with metadata (date, outcome, relevance score) into a structured prompt following Mistral's instruction format.</p>
        </div>
        <div class="callout callout-warning">
            <h4 class="font-medium mb-2">Stage 3: LLM Prediction</h4>
            <p class="text-sm text-gray-600">QLoRA fine-tuning adapts the frozen Mistral-7B model with only 0.1% trainable parameters, enabling efficient domain adaptation.</p>
        </div>
    </div>
</section>

<!-- Section 2: Task Formulation -->
<section id="task-formulation" class="card card-academic p-8 mb-8">
    <h2 class="section-header text-2xl font-bold">
        <span class="section-number">2</span>
        Task Formulation
    </h2>

    <div class="grid md:grid-cols-2 gap-8 mb-6">
        <div>
            <h3 class="font-semibold mb-4">Formal Definition</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm mb-4">
                <pre>
Legal Outcome Prediction Task:

Input:  C = (T, M, G)
  where:
    T = case text (opinion, arguments)
    M = metadata (date, court, parties)
    G = citation subgraph context

Output: y âˆˆ {petitioner, respondent}

Objective: Learn f: (T, M, G) â†’ y
  that maximizes P(y | T, M, G)
                </pre>
            </div>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Label Definition (SCDB)</h3>
            <table class="w-full text-sm">
                <thead>
                    <tr class="border-b">
                        <th class="py-2 text-left">Label</th>
                        <th class="py-2 text-left">Definition</th>
                        <th class="py-2 text-center">Frequency</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="border-b">
                        <td class="py-2 font-mono">petitioner (1)</td>
                        <td class="py-2 text-gray-600">Party bringing appeal wins</td>
                        <td class="py-2 text-center">57%</td>
                    </tr>
                    <tr>
                        <td class="py-2 font-mono">respondent (0)</td>
                        <td class="py-2 text-gray-600">Party responding wins</td>
                        <td class="py-2 text-center">43%</td>
                    </tr>
                </tbody>
            </table>
            <p class="text-sm text-gray-500 mt-4">
                Labels derived from SCDB <code>partyWinning</code> variable. Cases with unclear outcomes (remands, mixed decisions) excluded.
            </p>
        </div>
    </div>

    <div class="bg-blue-50 p-6 rounded-lg">
        <h3 class="font-semibold mb-4">Why This Formulation?</h3>
        <div class="grid md:grid-cols-2 gap-6 text-sm">
            <div>
                <h4 class="font-medium text-blue-800 mb-2">Design Choices</h4>
                <ul class="space-y-2 text-gray-600">
                    <li><strong>Binary classification:</strong> Simplifies evaluation; multi-class (unanimous, split, remand) is future work</li>
                    <li><strong>Case-level prediction:</strong> Predicts overall winner, not issue-by-issue outcomes</li>
                    <li><strong>Post-hoc prediction:</strong> Uses full opinion text (retrospective analysis, not pre-decision forecasting)</li>
                </ul>
            </div>
            <div>
                <h4 class="font-medium text-blue-800 mb-2">Comparison to Prior Work</h4>
                <ul class="space-y-2 text-gray-600">
                    <li><strong>Katz et al. (2017):</strong> Used pre-argument features only (true forecasting)</li>
                    <li><strong>Chalkidis et al. (2019):</strong> ECHR violation prediction (similar setup)</li>
                    <li><strong>Ours:</strong> Full text + citation context (maximum information)</li>
                </ul>
            </div>
        </div>
    </div>
</section>

<!-- Section 3: GraphSAGE -->
<section id="graphsage" class="card card-academic p-8 mb-8">
    <h2 class="section-header text-2xl font-bold">
        <span class="section-number">3</span>
        GraphSAGE Embeddings
    </h2>

    <p class="text-gray-600 mb-6">
        We employ GraphSAGE (Hamilton et al., 2017) to learn inductive node representations that capture both textual semantics and citation graph structure. Unlike transductive methods (e.g., DeepWalk, Node2Vec), GraphSAGE can embed unseen nodes at inference time.
    </p>

    <div class="grid md:grid-cols-2 gap-8 mb-8">
        <div>
            <h3 class="font-semibold mb-4">Architecture Configuration</h3>
            <table class="w-full text-sm">
                <thead>
                    <tr class="border-b bg-gray-50">
                        <th class="py-2 px-3 text-left">Parameter</th>
                        <th class="py-2 px-3 text-left">Value</th>
                        <th class="py-2 px-3 text-left">Justification</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="border-b">
                        <td class="py-2 px-3 text-gray-600">Input Dimensions</td>
                        <td class="py-2 px-3 font-mono">384</td>
                        <td class="py-2 px-3 text-gray-500 text-xs">all-MiniLM-L6-v2</td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 px-3 text-gray-600">Hidden Dimensions</td>
                        <td class="py-2 px-3 font-mono">256</td>
                        <td class="py-2 px-3 text-gray-500 text-xs">2Ã— compression</td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 px-3 text-gray-600">Output Dimensions</td>
                        <td class="py-2 px-3 font-mono">128</td>
                        <td class="py-2 px-3 text-gray-500 text-xs">Retrieval efficiency</td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 px-3 text-gray-600">Number of Layers</td>
                        <td class="py-2 px-3 font-mono">2</td>
                        <td class="py-2 px-3 text-gray-500 text-xs">2-hop neighborhood</td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 px-3 text-gray-600">Aggregator</td>
                        <td class="py-2 px-3 font-mono">MEAN</td>
                        <td class="py-2 px-3 text-gray-500 text-xs">Permutation invariant</td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 px-3 text-gray-600">Activation</td>
                        <td class="py-2 px-3 font-mono">ReLU</td>
                        <td class="py-2 px-3 text-gray-500 text-xs">Standard choice</td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 px-3 text-gray-600">Dropout</td>
                        <td class="py-2 px-3 font-mono">0.3</td>
                        <td class="py-2 px-3 text-gray-500 text-xs">Regularization</td>
                    </tr>
                    <tr>
                        <td class="py-2 px-3 text-gray-600">Normalization</td>
                        <td class="py-2 px-3 font-mono">L2</td>
                        <td class="py-2 px-3 text-gray-500 text-xs">Unit sphere</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Message Passing Formulation</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm mb-4">
                <pre>
GraphSAGE Layer (Hamilton et al., 2017):

AGGREGATE:
  a_v^(k) = MEAN({h_u^(k-1) : u âˆˆ N(v)})

COMBINE:
  h_v^(k) = Ïƒ(W^(k) Â· CONCAT(h_v^(k-1), a_v^(k)))

With L2 normalization:
  h_v^(k) = h_v^(k) / ||h_v^(k)||â‚‚

Where:
  h_v^(0) = x_v (initial node features)
  N(v) = {u : (u,v) âˆˆ E} (cited cases)
  Ïƒ = ReLU activation
  W^(k) âˆˆ â„^{d_k Ã— 2d_{k-1}}
                </pre>
            </div>
            <p class="text-sm text-gray-500">
                The MEAN aggregator provides permutation invariance over neighborhoods. L2 normalization ensures embeddings lie on the unit hypersphere for cosine similarity retrieval.
            </p>
        </div>
    </div>

    <div class="grid md:grid-cols-2 gap-8 mb-6">
        <div>
            <h3 class="font-semibold mb-4">Node Feature Initialization</h3>
            <div class="bg-yellow-50 p-4 rounded-lg">
                <table class="w-full text-sm">
                    <tr class="border-b">
                        <td class="py-2 text-gray-600">Text Embedding</td>
                        <td class="py-2 font-mono">384-dim</td>
                        <td class="py-2 text-gray-500">sentence-transformers/all-MiniLM-L6-v2</td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 text-gray-600">Temporal Feature</td>
                        <td class="py-2 font-mono">1-dim</td>
                        <td class="py-2 text-gray-500">Normalized year: (year - 1946) / 77</td>
                    </tr>
                    <tr>
                        <td class="py-2 text-gray-600">Total Input</td>
                        <td class="py-2 font-mono">385-dim</td>
                        <td class="py-2 text-gray-500">Concatenated features</td>
                    </tr>
                </table>
            </div>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Neighborhood Sampling</h3>
            <div class="bg-purple-50 p-4 rounded-lg">
                <table class="w-full text-sm">
                    <tr class="border-b">
                        <td class="py-2 text-gray-600">Layer 1 neighbors</td>
                        <td class="py-2 font-mono">25</td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 text-gray-600">Layer 2 neighbors</td>
                        <td class="py-2 font-mono">10</td>
                    </tr>
                    <tr>
                        <td class="py-2 text-gray-600">Total sampled</td>
                        <td class="py-2 font-mono">â‰¤ 275 nodes/case</td>
                    </tr>
                </table>
                <p class="text-xs text-gray-500 mt-2">
                    Uniform sampling from neighbors. Capped to limit memory during training.
                </p>
            </div>
        </div>
    </div>

    <div class="bg-gray-900 text-green-400 p-6 rounded-lg">
        <h3 class="font-semibold mb-4 text-white">PyTorch Geometric Implementation</h3>
        <pre class="text-sm overflow-x-auto">
<span class="text-purple-400">import</span> torch
<span class="text-purple-400">import</span> torch.nn <span class="text-purple-400">as</span> nn
<span class="text-purple-400">from</span> torch_geometric.nn <span class="text-purple-400">import</span> SAGEConv

<span class="text-purple-400">class</span> <span class="text-yellow-400">LegalGraphSAGE</span>(nn.Module):
    <span class="text-purple-400">def</span> <span class="text-blue-400">__init__</span>(self, in_dim=<span class="text-cyan-400">385</span>, hidden_dim=<span class="text-cyan-400">256</span>, out_dim=<span class="text-cyan-400">128</span>, dropout=<span class="text-cyan-400">0.3</span>):
        <span class="text-purple-400">super</span>().__init__()
        self.conv1 = SAGEConv(in_dim, hidden_dim, normalize=<span class="text-cyan-400">True</span>)
        self.conv2 = SAGEConv(hidden_dim, out_dim, normalize=<span class="text-cyan-400">True</span>)
        self.dropout = nn.Dropout(dropout)

    <span class="text-purple-400">def</span> <span class="text-blue-400">forward</span>(self, x, edge_index):
        <span class="text-gray-500"># Layer 1: input â†’ hidden</span>
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)

        <span class="text-gray-500"># Layer 2: hidden â†’ output</span>
        x = self.conv2(x, edge_index)

        <span class="text-gray-500"># L2 normalize for cosine similarity</span>
        x = F.normalize(x, p=<span class="text-cyan-400">2</span>, dim=<span class="text-cyan-400">1</span>)
        <span class="text-purple-400">return</span> x
        </pre>
    </div>
</section>

<!-- Section 4: QLoRA Fine-tuning -->
<section id="qlora" class="card card-academic p-8 mb-8">
    <h2 class="section-header text-2xl font-bold">
        <span class="section-number">4</span>
        QLoRA Fine-tuning
    </h2>

    <p class="text-gray-600 mb-6">
        We employ QLoRA (Dettmers et al., 2023) to efficiently fine-tune Mistral-7B-Instruct for legal outcome prediction. QLoRA combines 4-bit quantization with Low-Rank Adaptation (LoRA; Hu et al., 2022), reducing memory requirements by ~4Ã— while maintaining full fine-tuning performance.
    </p>

    <div class="grid md:grid-cols-2 gap-8 mb-8">
        <div>
            <h3 class="font-semibold mb-4">Base Model: Mistral-7B-Instruct-v0.3</h3>
            <table class="w-full text-sm">
                <thead>
                    <tr class="border-b bg-gray-50">
                        <th class="py-2 px-3 text-left">Attribute</th>
                        <th class="py-2 px-3 text-left">Value</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="border-b"><td class="py-2 px-3 text-gray-600">Parameters</td><td class="py-2 px-3 font-mono">7.24B</td></tr>
                    <tr class="border-b"><td class="py-2 px-3 text-gray-600">Architecture</td><td class="py-2 px-3 font-mono">Transformer decoder</td></tr>
                    <tr class="border-b"><td class="py-2 px-3 text-gray-600">Context Length</td><td class="py-2 px-3 font-mono">32,768 tokens</td></tr>
                    <tr class="border-b"><td class="py-2 px-3 text-gray-600">Hidden Dimension</td><td class="py-2 px-3 font-mono">4,096</td></tr>
                    <tr class="border-b"><td class="py-2 px-3 text-gray-600">Attention Heads</td><td class="py-2 px-3 font-mono">32</td></tr>
                    <tr class="border-b"><td class="py-2 px-3 text-gray-600">Layers</td><td class="py-2 px-3 font-mono">32</td></tr>
                    <tr class="border-b"><td class="py-2 px-3 text-gray-600">Vocabulary</td><td class="py-2 px-3 font-mono">32,000</td></tr>
                    <tr><td class="py-2 px-3 text-gray-600">License</td><td class="py-2 px-3 font-mono">Apache 2.0</td></tr>
                </tbody>
            </table>
        </div>

        <div>
            <h3 class="font-semibold mb-4">QLoRA Configuration</h3>
            <table class="w-full text-sm">
                <thead>
                    <tr class="border-b bg-gray-50">
                        <th class="py-2 px-3 text-left">Parameter</th>
                        <th class="py-2 px-3 text-left">Value</th>
                        <th class="py-2 px-3 text-left">Notes</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="border-b"><td class="py-2 px-3 text-gray-600">Quantization</td><td class="py-2 px-3 font-mono">4-bit NF4</td><td class="py-2 px-3 text-gray-500 text-xs">NormalFloat</td></tr>
                    <tr class="border-b"><td class="py-2 px-3 text-gray-600">Double Quant</td><td class="py-2 px-3 font-mono">True</td><td class="py-2 px-3 text-gray-500 text-xs">Quantize constants</td></tr>
                    <tr class="border-b"><td class="py-2 px-3 text-gray-600">Compute dtype</td><td class="py-2 px-3 font-mono">bfloat16</td><td class="py-2 px-3 text-gray-500 text-xs">Mixed precision</td></tr>
                    <tr class="border-b"><td class="py-2 px-3 text-gray-600">LoRA Rank (r)</td><td class="py-2 px-3 font-mono">16</td><td class="py-2 px-3 text-gray-500 text-xs">Low-rank dim</td></tr>
                    <tr class="border-b"><td class="py-2 px-3 text-gray-600">LoRA Alpha (Î±)</td><td class="py-2 px-3 font-mono">32</td><td class="py-2 px-3 text-gray-500 text-xs">Scaling = Î±/r</td></tr>
                    <tr class="border-b"><td class="py-2 px-3 text-gray-600">LoRA Dropout</td><td class="py-2 px-3 font-mono">0.05</td><td class="py-2 px-3 text-gray-500 text-xs">Regularization</td></tr>
                    <tr><td class="py-2 px-3 text-gray-600">Trainable</td><td class="py-2 px-3 font-mono">~7M (0.1%)</td><td class="py-2 px-3 text-gray-500 text-xs">Of 7.24B total</td></tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="bg-gray-100 p-6 rounded-lg mb-6">
        <h3 class="font-semibold mb-4">LoRA Mathematical Formulation</h3>
        <div class="grid md:grid-cols-2 gap-6">
            <div class="font-mono text-sm">
                <pre>
Low-Rank Adaptation (Hu et al., 2022):

Original: h = Wâ‚€x
LoRA:     h = Wâ‚€x + Î”Wx
              = Wâ‚€x + BAx

Where:
  Wâ‚€ âˆˆ â„^{dÃ—k} (frozen pretrained)
  B âˆˆ â„^{dÃ—r} (trainable)
  A âˆˆ â„^{rÃ—k} (trainable)
  r << min(d, k) (low rank)

Scaling: Î”W = (Î±/r) Â· BA
  with Î± = 32, r = 16 â†’ scale = 2
                </pre>
            </div>
            <div>
                <h4 class="font-medium mb-2">Target Modules</h4>
                <div class="space-y-2 text-sm">
                    <div class="flex items-center">
                        <span class="w-24 text-gray-600">Attention:</span>
                        <code class="bg-white px-2 py-1 rounded">q_proj, k_proj, v_proj, o_proj</code>
                    </div>
                    <div class="flex items-center">
                        <span class="w-24 text-gray-600">MLP:</span>
                        <code class="bg-white px-2 py-1 rounded">gate_proj, up_proj, down_proj</code>
                    </div>
                </div>
                <p class="text-sm text-gray-500 mt-4">
                    We apply LoRA to all linear layers in both attention and MLP blocks, following Dettmers et al. (2023) recommendation for QLoRA.
                </p>
                <div class="mt-4 p-3 bg-yellow-100 rounded text-sm">
                    <strong>Memory savings:</strong> 4-bit quantization reduces model memory from ~14GB to ~4GB, enabling training on single A100-40GB.
                </div>
            </div>
        </div>
    </div>

    <div class="grid md:grid-cols-2 gap-8">
        <div>
            <h3 class="font-semibold mb-4">Training Hyperparameters</h3>
            <table class="w-full text-sm">
                <tbody>
                    <tr class="border-b"><td class="py-2 text-gray-600">Learning Rate</td><td class="py-2 font-mono">2e-4</td></tr>
                    <tr class="border-b"><td class="py-2 text-gray-600">LR Scheduler</td><td class="py-2 font-mono">Cosine with warmup</td></tr>
                    <tr class="border-b"><td class="py-2 text-gray-600">Warmup Ratio</td><td class="py-2 font-mono">0.1 (10%)</td></tr>
                    <tr class="border-b"><td class="py-2 text-gray-600">Batch Size</td><td class="py-2 font-mono">4</td></tr>
                    <tr class="border-b"><td class="py-2 text-gray-600">Gradient Accumulation</td><td class="py-2 font-mono">4 steps</td></tr>
                    <tr class="border-b"><td class="py-2 text-gray-600">Effective Batch Size</td><td class="py-2 font-mono">16</td></tr>
                    <tr class="border-b"><td class="py-2 text-gray-600">Epochs</td><td class="py-2 font-mono">3</td></tr>
                    <tr class="border-b"><td class="py-2 text-gray-600">Max Sequence Length</td><td class="py-2 font-mono">4,096 tokens</td></tr>
                    <tr class="border-b"><td class="py-2 text-gray-600">Optimizer</td><td class="py-2 font-mono">AdamW (8-bit)</td></tr>
                    <tr class="border-b"><td class="py-2 text-gray-600">Weight Decay</td><td class="py-2 font-mono">0.01</td></tr>
                    <tr><td class="py-2 text-gray-600">Gradient Clipping</td><td class="py-2 font-mono">1.0</td></tr>
                </tbody>
            </table>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Classification Head</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm mb-4">
                <pre>
Sequence Classification Architecture:

1. Extract last token hidden state:
   h_last = LLM(input_ids)[:, -1, :]
   h_last âˆˆ â„^{batch Ã— 4096}

2. Linear projection:
   logits = W_cls Â· h_last + b_cls
   W_cls âˆˆ â„^{2 Ã— 4096}
   logits âˆˆ â„^{batch Ã— 2}

3. Softmax probabilities:
   P(y|x) = softmax(logits)
                </pre>
            </div>
            <p class="text-sm text-gray-500">
                We use the last token representation following standard practice for causal LM classification (Radford et al., 2019).
            </p>
        </div>
    </div>
</section>

<!-- Section 5: Loss Functions -->
<section id="loss-functions" class="card card-academic p-8 mb-8">
    <h2 class="section-header text-2xl font-bold">
        <span class="section-number">5</span>
        Loss Functions
    </h2>

    <p class="text-gray-600 mb-6">
        Training objectives for the classification task and GraphSAGE link prediction, with regularization techniques for improved generalization.
    </p>

    <div class="grid md:grid-cols-2 gap-8 mb-8">
        <div>
            <h3 class="font-semibold mb-4">Cross-Entropy Loss</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm mb-4">
                <pre>
Standard Cross-Entropy:

â„’_CE = -âˆ‘_{i=1}^{N} âˆ‘_{c=1}^{C} y_{i,c} Â· log(p_{i,c})

For binary classification (C=2):

â„’_CE = -1/N âˆ‘_{i=1}^{N} [y_iÂ·log(p_i) + (1-y_i)Â·log(1-p_i)]

Where:
  N = number of samples
  C = number of classes (2)
  y_{i,c} = ground truth (one-hot)
  p_{i,c} = predicted probability
                </pre>
            </div>
            <p class="text-sm text-gray-500">
                Cross-entropy measures the divergence between predicted and true distributions. Minimizing CE is equivalent to maximum likelihood estimation (Goodfellow et al., 2016).
            </p>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Label Smoothing Regularization</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm mb-4">
                <pre>
Label Smoothing (Szegedy et al., 2016):

Smoothed targets:
  y'_{i,c} = y_{i,c}Â·(1-Îµ) + Îµ/C

With Îµ = 0.1:
  Hard: [1, 0] â†’ Soft: [0.95, 0.05]
  Hard: [0, 1] â†’ Soft: [0.05, 0.95]

Equivalent loss:
  â„’_LS = (1-Îµ)Â·â„’_CE(y,p) + ÎµÂ·H(u,p)

Where H(u,p) is CE with uniform dist.
                </pre>
            </div>
            <p class="text-sm text-gray-500">
                Label smoothing prevents overconfident predictions, improving calibration and generalization (MÃ¼ller et al., 2019).
            </p>
        </div>
    </div>

    <div class="bg-blue-50 p-6 rounded-lg mb-6">
        <h3 class="font-semibold mb-4">Link Prediction Loss (GraphSAGE)</h3>
        <div class="font-mono text-sm mb-4">
            <pre>
Contrastive Link Prediction Objective:

â„’_link = -âˆ‘_{(u,v)âˆˆE} log(Ïƒ(z_u^T Â· z_v)) - Q Â· ğ”¼_{v_nâˆ¼P_n} [log(Ïƒ(-z_u^T Â· z_{v_n}))]

Simplified binary cross-entropy form:

â„’_link = -1/|E| âˆ‘_{(u,v)âˆˆE} [log(Ïƒ(z_u^TÂ·z_v)) + âˆ‘_{j=1}^{Q} log(Ïƒ(-z_u^TÂ·z_{v_j}^-))]

Where:
  E = observed citation edges
  z_u, z_v = learned node embeddings (128-dim)
  Ïƒ = sigmoid function
  Q = negative samples per positive (Q=5)
  P_n(v) âˆ degree(v)^0.75 (negative distribution)
  v_j^- = sampled negative node
            </pre>
        </div>
        <div class="grid md:grid-cols-3 gap-4">
            <div class="bg-white p-3 rounded text-center">
                <div class="text-lg font-bold text-blue-600">Positive Term</div>
                <div class="text-sm text-gray-600">Maximize similarity for cited pairs</div>
            </div>
            <div class="bg-white p-3 rounded text-center">
                <div class="text-lg font-bold text-purple-600">Negative Term</div>
                <div class="text-sm text-gray-600">Minimize similarity for non-cited</div>
            </div>
            <div class="bg-white p-3 rounded text-center">
                <div class="text-lg font-bold text-green-600">Contrastive</div>
                <div class="text-sm text-gray-600">Learn discriminative embeddings</div>
            </div>
        </div>
    </div>

    <div class="grid md:grid-cols-2 gap-8">
        <div>
            <h3 class="font-semibold mb-4">Total Training Objective</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm">
                <pre>
Multi-task Loss:

â„’_total = â„’_classification + Î»Â·â„’_link

Where:
  â„’_classification: QLoRA fine-tuning loss
  â„’_link: GraphSAGE link prediction loss
  Î» = 0.1 (balancing coefficient)

Training schedule:
  1. Pre-train GraphSAGE (link prediction)
  2. Freeze GraphSAGE, train QLoRA
  3. Optional: joint fine-tuning (Î» > 0)
                </pre>
            </div>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Regularization Summary</h3>
            <table class="w-full text-sm">
                <thead>
                    <tr class="border-b bg-gray-50">
                        <th class="py-2 px-3 text-left">Technique</th>
                        <th class="py-2 px-3 text-left">Value</th>
                        <th class="py-2 px-3 text-left">Effect</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="border-b"><td class="py-2 px-3">Label Smoothing</td><td class="py-2 px-3 font-mono">Îµ=0.1</td><td class="py-2 px-3 text-gray-500">Calibration</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Weight Decay</td><td class="py-2 px-3 font-mono">0.01</td><td class="py-2 px-3 text-gray-500">L2 penalty</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Dropout (LoRA)</td><td class="py-2 px-3 font-mono">0.05</td><td class="py-2 px-3 text-gray-500">Adaptation</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Dropout (GNN)</td><td class="py-2 px-3 font-mono">0.3</td><td class="py-2 px-3 text-gray-500">Graph layers</td></tr>
                    <tr><td class="py-2 px-3">Gradient Clip</td><td class="py-2 px-3 font-mono">1.0</td><td class="py-2 px-3 text-gray-500">Stability</td></tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<!-- Section 6: Negative Sampling -->
<section id="negative-sampling" class="card card-academic p-8 mb-8">
    <h2 class="section-header text-2xl font-bold">
        <span class="section-number">6</span>
        Negative Sampling Strategy
    </h2>

    <p class="text-gray-600 mb-6">
        Effective negative sampling is crucial for learning discriminative graph embeddings. We employ degree-biased sampling with hard negative mining following best practices from knowledge graph embedding literature.
    </p>

    <div class="grid md:grid-cols-2 gap-8 mb-8">
        <div>
            <h3 class="font-semibold mb-4">Sampling Distribution</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm mb-4">
                <pre>
Negative Sampling Distribution:

P_n(v) âˆ degree(v)^Î±

With Î± = 0.75 (Mikolov et al., 2013):
  - Î± = 0: uniform sampling
  - Î± = 1: degree-proportional
  - Î± = 0.75: smoothed (empirically optimal)

Effect: Reduces sampling of rare nodes,
focuses on distinguishing similar cases.
                </pre>
            </div>
            <table class="w-full text-sm">
                <tr class="border-b"><td class="py-2 text-gray-600">Positive edges (train)</td><td class="py-2 font-mono">~120,000</td></tr>
                <tr class="border-b"><td class="py-2 text-gray-600">Negative ratio (Q)</td><td class="py-2 font-mono">5</td></tr>
                <tr class="border-b"><td class="py-2 text-gray-600">Sampling exponent (Î±)</td><td class="py-2 font-mono">0.75</td></tr>
                <tr><td class="py-2 text-gray-600">Training pairs/epoch</td><td class="py-2 font-mono">~720,000</td></tr>
            </table>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Hard Negative Mining</h3>
            <div class="bg-yellow-50 p-4 rounded-lg mb-4">
                <p class="text-sm text-gray-600 mb-2">
                    <strong>Hard negatives:</strong> Cases that are structurally or semantically close but not directly cited.
                </p>
                <div class="font-mono text-sm bg-white p-3 rounded">
                    <pre>
Hard negative criteria:
1. 2-hop neighbors (cited by same case)
2. Same legal issue area (SCDB code)
3. Temporal proximity (Â±5 years)
4. High text similarity (>0.7 cosine)
                    </pre>
                </div>
            </div>
            <h4 class="font-medium mb-2">Curriculum Schedule</h4>
            <table class="w-full text-sm">
                <tr class="border-b"><td class="py-2 text-gray-600">Epochs 1-2</td><td class="py-2 font-mono">100% random</td><td class="py-2 text-gray-500">Easy start</td></tr>
                <tr class="border-b"><td class="py-2 text-gray-600">Epochs 3-4</td><td class="py-2 font-mono">80% + 20% hard</td><td class="py-2 text-gray-500">Gradual</td></tr>
                <tr><td class="py-2 text-gray-600">Epochs 5+</td><td class="py-2 font-mono">60% + 40% hard</td><td class="py-2 text-gray-500">Full difficulty</td></tr>
            </table>
        </div>
    </div>

    <div class="bg-green-50 p-6 rounded-lg">
        <h3 class="font-semibold mb-4">Temporal Constraints</h3>
        <div class="grid md:grid-cols-2 gap-6">
            <div>
                <div class="font-mono text-sm bg-white p-4 rounded">
                    <pre>
Citation temporal constraint:

For edge (u, v) where u cites v:
  date(v) < date(u)  # v must precede u

This ensures:
  - No future citations (anachronistic)
  - Realistic precedent relationships
  - Proper train/test temporal split
                    </pre>
                </div>
            </div>
            <div>
                <h4 class="font-medium mb-2">Implications for Negative Sampling</h4>
                <ul class="text-sm text-gray-600 space-y-2">
                    <li>â€¢ Negatives must also respect temporal ordering</li>
                    <li>â€¢ Cannot sample future cases as negatives for historical ones</li>
                    <li>â€¢ Prevents temporal data leakage during training</li>
                </ul>
            </div>
        </div>
    </div>
</section>

<!-- Section 7: Hybrid Retrieval -->
<section id="hybrid-retrieval" class="card card-academic p-8 mb-8">
    <h2 class="section-header text-2xl font-bold">
        <span class="section-number">7</span>
        Hybrid Retrieval System
    </h2>

    <p class="text-gray-600 mb-6">
        Our retrieval system combines dense embedding similarity with sparse citation graph structure, following the hybrid retrieval paradigm (Karpukhin et al., 2020; Ma et al., 2021).
    </p>

    <div class="bg-blue-100 p-6 rounded-lg mb-8">
        <h3 class="font-semibold mb-4 text-center">Hybrid Scoring Function</h3>
        <div class="text-center font-mono text-lg mb-4">
            <strong>Score(q, d) = Î± Â· S_embed(q, d) + Î² Â· S_citation(q, d) + Î³ Â· S_text(q, d)</strong>
        </div>
        <div class="grid md:grid-cols-3 gap-6">
            <div class="bg-white p-4 rounded text-center">
                <div class="text-3xl font-bold text-blue-600">Î± = 0.40</div>
                <div class="font-medium">GraphSAGE Similarity</div>
                <div class="text-sm text-gray-500">Learned structural + semantic</div>
            </div>
            <div class="bg-white p-4 rounded text-center">
                <div class="text-3xl font-bold text-purple-600">Î² = 0.35</div>
                <div class="font-medium">Citation Proximity</div>
                <div class="text-sm text-gray-500">Graph distance signal</div>
            </div>
            <div class="bg-white p-4 rounded text-center">
                <div class="text-3xl font-bold text-green-600">Î³ = 0.25</div>
                <div class="font-medium">Text Similarity</div>
                <div class="text-sm text-gray-500">BM25 lexical matching</div>
            </div>
        </div>
    </div>

    <div class="grid md:grid-cols-3 gap-6 mb-8">
        <div>
            <h3 class="font-semibold mb-4">S_embed: Embedding Similarity</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm">
                <pre>
Cosine similarity of GraphSAGE embeddings:

S_embed(q, d) = cos(z_q, z_d)
              = (z_q Â· z_d) / (||z_q|| Â· ||z_d||)

Since embeddings are L2-normalized:
S_embed(q, d) = z_q Â· z_d  (dot product)

Range: [-1, 1] â†’ normalized to [0, 1]
                </pre>
            </div>
        </div>

        <div>
            <h3 class="font-semibold mb-4">S_citation: Graph Proximity</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm">
                <pre>
Inverse shortest path distance:

S_citation(q, d) = 1 / (1 + dist(q, d))

Where dist(q, d) is shortest path
in the citation graph.

Special cases:
  - Direct citation: dist=1 â†’ S=0.5
  - 2-hop: dist=2 â†’ S=0.33
  - Unreachable: dist=âˆ â†’ S=0
                </pre>
            </div>
        </div>

        <div>
            <h3 class="font-semibold mb-4">S_text: BM25 Similarity</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm">
                <pre>
BM25 (Robertson et al., 1995):

S_text(q, d) = âˆ‘_{tâˆˆq} IDF(t) Â·
  (f(t,d)Â·(kâ‚+1)) /
  (f(t,d) + kâ‚Â·(1-b+bÂ·|d|/avgdl))

Parameters:
  kâ‚ = 1.2 (term frequency saturation)
  b = 0.75 (length normalization)
                </pre>
            </div>
        </div>
    </div>

    <div class="grid md:grid-cols-2 gap-8">
        <div>
            <h3 class="font-semibold mb-4">Retrieval Algorithm</h3>
            <div class="bg-gray-900 text-green-400 p-4 rounded-lg font-mono text-sm">
                <pre>
<span class="text-purple-400">def</span> <span class="text-blue-400">retrieve_precedents</span>(query_case, k=<span class="text-cyan-400">5</span>):
    <span class="text-gray-500"># Stage 1: Candidate generation (fast)</span>
    candidates = []
    candidates += ann_search(query_embedding, n=<span class="text-cyan-400">100</span>)
    candidates += citation_neighbors(query, hops=<span class="text-cyan-400">2</span>)
    candidates += bm25_search(query_text, n=<span class="text-cyan-400">50</span>)
    candidates = deduplicate(candidates)

    <span class="text-gray-500"># Stage 2: Hybrid re-ranking</span>
    scores = []
    <span class="text-purple-400">for</span> doc <span class="text-purple-400">in</span> candidates:
        s = Î± * embed_sim(query, doc)
        s += Î² * citation_proximity(query, doc)
        s += Î³ * bm25_score(query, doc)
        scores.append((doc, s))

    <span class="text-gray-500"># Stage 3: Return top-k</span>
    <span class="text-purple-400">return</span> sorted(scores, reverse=<span class="text-cyan-400">True</span>)[:k]
                </pre>
            </div>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Weight Ablation Results</h3>
            <table class="w-full text-sm mb-4">
                <thead>
                    <tr class="border-b bg-gray-50">
                        <th class="py-2 px-3 text-left">Configuration</th>
                        <th class="py-2 px-3 text-center">AUROC</th>
                        <th class="py-2 px-3 text-center">Î”</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="border-b"><td class="py-2 px-3">Î±=1.0 (embed only)</td><td class="py-2 px-3 text-center">0.76</td><td class="py-2 px-3 text-center text-red-500">-0.04</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Î²=1.0 (citation only)</td><td class="py-2 px-3 text-center">0.77</td><td class="py-2 px-3 text-center text-red-500">-0.03</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Î³=1.0 (BM25 only)</td><td class="py-2 px-3 text-center">0.74</td><td class="py-2 px-3 text-center text-red-500">-0.06</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Î±=0.5, Î²=0.5</td><td class="py-2 px-3 text-center">0.78</td><td class="py-2 px-3 text-center text-red-500">-0.02</td></tr>
                    <tr class="bg-green-50"><td class="py-2 px-3 font-medium">Î±=0.4, Î²=0.35, Î³=0.25</td><td class="py-2 px-3 text-center font-bold">0.80</td><td class="py-2 px-3 text-center">â€”</td></tr>
                </tbody>
            </table>
            <p class="text-sm text-gray-500">
                Optimal weights found via grid search on validation set. Three-signal combination outperforms any single signal.
            </p>
        </div>
    </div>
</section>

<!-- Section 8: Embedding Fusion -->
<section id="embedding-fusion" class="card card-academic p-8 mb-8">
    <h2 class="section-header text-2xl font-bold">
        <span class="section-number">8</span>
        Embedding Fusion Architecture
    </h2>

    <p class="text-gray-600 mb-6">
        Multi-modal representation combining semantic text features with structural graph information through learned fusion layers.
    </p>

    <div class="bg-gray-100 p-6 rounded-lg mb-8">
        <h3 class="font-semibold mb-4 text-center">Fusion Architecture Diagram</h3>
        <div class="font-mono text-sm overflow-x-auto">
            <pre class="text-center">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Case Text       â”‚          â”‚   Citation Graph    â”‚
â”‚   (full opinion)    â”‚          â”‚   (node + edges)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                 â”‚
           â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sentence-BERT     â”‚          â”‚     GraphSAGE       â”‚
â”‚  all-MiniLM-L6-v2   â”‚          â”‚    2-layer GNN      â”‚
â”‚     (frozen)        â”‚          â”‚    (trained)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                 â”‚
           â–¼                                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  e_text     â”‚                  â”‚  e_graph    â”‚
    â”‚  (384-dim)  â”‚                  â”‚  (128-dim)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                 â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    Concatenation    â”‚
              â”‚  [e_text; e_graph]  â”‚
              â”‚     (512-dim)       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Fusion MLP        â”‚
              â”‚  512 â†’ 256 â†’ 128    â”‚
              â”‚   ReLU + Dropout    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  e_fused (128-dim)  â”‚
              â”‚   L2 normalized     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            </pre>
        </div>
    </div>

    <div class="grid md:grid-cols-2 gap-8 mb-8">
        <div>
            <h3 class="font-semibold mb-4">Concatenation Fusion</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm mb-4">
                <pre>
Simple Concatenation:

e_concat = [e_text; e_graph]
         = [384-dim; 128-dim]
         = 512-dim

Fusion MLP:
h1 = ReLU(W1 Â· e_concat + b1)  # 512â†’256
h1 = Dropout(h1, p=0.2)
h2 = ReLU(W2 Â· h1 + b2)        # 256â†’128
e_fused = L2_normalize(h2)

Where:
  W1 âˆˆ â„^{256Ã—512}, b1 âˆˆ â„^256
  W2 âˆˆ â„^{128Ã—256}, b2 âˆˆ â„^128
                </pre>
            </div>
            <p class="text-sm text-gray-500">
                Concatenation preserves all information from both modalities. The MLP learns non-linear interactions (Baltrusaitis et al., 2019).
            </p>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Alternative: Gated Fusion</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm mb-4">
                <pre>
Gated Fusion (Arevalo et al., 2017):

g = Ïƒ(W_g Â· [e_text; e_graph] + b_g)
e_fused = g âŠ™ tanh(W_tÂ·e_text) +
          (1-g) âŠ™ tanh(W_gÂ·e_graph)

Where:
  g âˆˆ â„^d: learned gating vector
  Ïƒ: sigmoid activation
  âŠ™: element-wise multiplication

Advantage: Adaptive weighting per
dimension based on input content.
                </pre>
            </div>
            <p class="text-sm text-gray-500">
                Gated fusion allows the model to dynamically weight each modality. We found concatenation performs comparably with simpler implementation.
            </p>
        </div>
    </div>

    <div class="bg-purple-50 p-6 rounded-lg">
        <h3 class="font-semibold mb-4">Fusion Ablation Results</h3>
        <div class="grid md:grid-cols-4 gap-4">
            <div class="bg-white p-4 rounded text-center">
                <div class="text-xl font-bold text-gray-600">0.74</div>
                <div class="text-sm text-gray-500">Text Only</div>
            </div>
            <div class="bg-white p-4 rounded text-center">
                <div class="text-xl font-bold text-gray-600">0.76</div>
                <div class="text-sm text-gray-500">Graph Only</div>
            </div>
            <div class="bg-white p-4 rounded text-center">
                <div class="text-xl font-bold text-blue-600">0.79</div>
                <div class="text-sm text-gray-500">Concatenation</div>
            </div>
            <div class="bg-white p-4 rounded text-center border-2 border-green-500">
                <div class="text-xl font-bold text-green-600">0.80</div>
                <div class="text-sm text-gray-500">Concat + MLP</div>
            </div>
        </div>
        <p class="text-sm text-gray-600 mt-4 text-center">
            Fusion provides +6% AUROC over text-only and +4% over graph-only baselines.
        </p>
    </div>
</section>

<!-- Section 9: Prompt Template -->
<section id="prompt-template" class="card card-academic p-8 mb-8">
    <h2 class="section-header text-2xl font-bold">
        <span class="section-number">9</span>
        Prompt Engineering
    </h2>

    <p class="text-gray-600 mb-6">
        The prompt template structures retrieved precedents with metadata to enable effective in-context learning. We follow Mistral's instruction format with careful attention to token budget management.
    </p>

    <div class="bg-gray-900 text-green-400 p-6 rounded-lg mb-8 overflow-x-auto">
        <h3 class="font-semibold mb-4 text-white">Full Prompt Template</h3>
        <pre class="text-sm">
<span class="text-purple-400">[INST]</span> You are a legal expert specializing in U.S. Supreme Court case analysis.
Your task is to predict the outcome of a case based on its content and relevant precedents.

<span class="text-yellow-400">## Case to Analyze</span>
<span class="text-cyan-400">Case Name:</span> {case_name}
<span class="text-cyan-400">Docket Number:</span> {docket}
<span class="text-cyan-400">Decision Date:</span> {date}
<span class="text-cyan-400">Legal Issue Area:</span> {issue_area}

<span class="text-cyan-400">Case Text (Opinion):</span>
{case_text_truncated}

<span class="text-yellow-400">## Relevant Precedents</span>
The following cases have been identified as relevant based on citation patterns and semantic similarity:

<span class="text-gray-500">{for i, precedent in enumerate(retrieved_cases, 1)}</span>
<span class="text-cyan-400">### Precedent {i}: {precedent.name} ({precedent.year})</span>
<span class="text-cyan-400">Relevance Score:</span> {precedent.score:.2f}
<span class="text-cyan-400">Outcome:</span> {precedent.outcome}
<span class="text-cyan-400">Citation Distance:</span> {precedent.citation_distance} hops

<span class="text-cyan-400">Key Excerpt:</span>
{precedent.text_excerpt}

<span class="text-gray-500">{endfor}</span>

<span class="text-yellow-400">## Task</span>
Based on the case text and the patterns observed in relevant precedents, predict whether
the <span class="text-green-400">PETITIONER</span> (party bringing the appeal) or <span class="text-red-400">RESPONDENT</span> (party responding) will win.

Consider:
1. How the legal issues align with precedent outcomes
2. The strength of citation relationships
3. The evolution of legal doctrine over time

<span class="text-cyan-400">Prediction:</span> <span class="text-purple-400">[/INST]</span>
        </pre>
    </div>

    <div class="grid md:grid-cols-2 gap-8 mb-6">
        <div>
            <h3 class="font-semibold mb-4">Token Budget Management</h3>
            <table class="w-full text-sm">
                <thead>
                    <tr class="border-b bg-gray-50">
                        <th class="py-2 px-3 text-left">Component</th>
                        <th class="py-2 px-3 text-center">Tokens</th>
                        <th class="py-2 px-3 text-center">%</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="border-b"><td class="py-2 px-3">System prompt</td><td class="py-2 px-3 text-center font-mono">~200</td><td class="py-2 px-3 text-center">1%</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Query case text</td><td class="py-2 px-3 text-center font-mono">~5,000</td><td class="py-2 px-3 text-center">25%</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Precedent 1</td><td class="py-2 px-3 text-center font-mono">~3,000</td><td class="py-2 px-3 text-center">15%</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Precedent 2</td><td class="py-2 px-3 text-center font-mono">~3,000</td><td class="py-2 px-3 text-center">15%</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Precedent 3</td><td class="py-2 px-3 text-center font-mono">~3,000</td><td class="py-2 px-3 text-center">15%</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Precedent 4</td><td class="py-2 px-3 text-center font-mono">~3,000</td><td class="py-2 px-3 text-center">15%</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Precedent 5</td><td class="py-2 px-3 text-center font-mono">~3,000</td><td class="py-2 px-3 text-center">15%</td></tr>
                    <tr class="bg-blue-50"><td class="py-2 px-3 font-medium">Total</td><td class="py-2 px-3 text-center font-mono font-bold">~20,200</td><td class="py-2 px-3 text-center">62%</td></tr>
                </tbody>
            </table>
            <p class="text-sm text-gray-500 mt-2">
                Remaining 38% of 32K context reserved for model generation and safety margin.
            </p>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Truncation Strategy</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm mb-4">
                <pre>
Hierarchical truncation:

1. Case text: truncate to 5000 tokens
   - Keep first 2500 (background)
   - Keep last 2500 (holding/conclusion)

2. Precedent excerpts: 3000 tokens each
   - Prioritize holding sections
   - Include key cited passages

3. If over budget:
   - Reduce precedent count (k=5â†’4â†’3)
   - Further truncate excerpts
                </pre>
            </div>
            <p class="text-sm text-gray-500">
                We preserve case beginnings and endings which typically contain facts and holdings respectively.
            </p>
        </div>
    </div>

    <div class="bg-yellow-50 p-6 rounded-lg">
        <h3 class="font-semibold mb-4">Prompt Engineering Ablations</h3>
        <div class="grid md:grid-cols-2 gap-6">
            <table class="w-full text-sm">
                <thead>
                    <tr class="border-b">
                        <th class="py-2 text-left">Variant</th>
                        <th class="py-2 text-center">AUROC</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="border-b"><td class="py-2">No system prompt</td><td class="py-2 text-center">0.77</td></tr>
                    <tr class="border-b"><td class="py-2">No precedent metadata</td><td class="py-2 text-center">0.78</td></tr>
                    <tr class="border-b"><td class="py-2">Random precedent order</td><td class="py-2 text-center">0.79</td></tr>
                    <tr class="border-b"><td class="py-2">No relevance scores</td><td class="py-2 text-center">0.79</td></tr>
                    <tr class="bg-green-100"><td class="py-2 font-medium">Full template</td><td class="py-2 text-center font-bold">0.80</td></tr>
                </tbody>
            </table>
            <div>
                <h4 class="font-medium mb-2">Key Findings</h4>
                <ul class="text-sm text-gray-600 space-y-2">
                    <li>â€¢ System prompt improves consistency (+3%)</li>
                    <li>â€¢ Precedent metadata provides useful signal (+2%)</li>
                    <li>â€¢ Ordering by relevance score marginally helps (+1%)</li>
                    <li>â€¢ Explicit relevance scores aid attention (+1%)</li>
                </ul>
            </div>
        </div>
    </div>
</section>

<!-- Section 10: Attention Analysis -->
<section id="attention-analysis" class="card card-academic p-8 mb-8">
    <h2 class="section-header text-2xl font-bold">
        <span class="section-number">10</span>
        Attention Analysis & Interpretability
    </h2>

    <p class="text-gray-600 mb-6">
        Understanding which precedents and text spans the model attends to provides interpretability for legal practitioners and validates that the model learns meaningful legal reasoning patterns.
    </p>

    <div class="grid md:grid-cols-2 gap-8 mb-8">
        <div>
            <h3 class="font-semibold mb-4">Attention Extraction Method</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm mb-4">
                <pre>
Multi-Head Attention Aggregation:

For layer L, head h:
  A^{(L,h)} = softmax(QK^T / âˆšd_k)

Attention to precedent p:
  Î±_p = (1/H) âˆ‘_{h=1}^{H} âˆ‘_{tâˆˆT_p} A^{(L,h)}_{cls,t}

Where:
  H = 32 attention heads
  T_p = token positions for precedent p
  cls = classification token (last)
  L = 32 (final layer)
                </pre>
            </div>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Attribution Methods</h3>
            <table class="w-full text-sm">
                <thead>
                    <tr class="border-b bg-gray-50">
                        <th class="py-2 px-3 text-left">Method</th>
                        <th class="py-2 px-3 text-left">Formula</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="border-b">
                        <td class="py-2 px-3">Attention Rollout</td>
                        <td class="py-2 px-3 font-mono text-xs">Ä€ = âˆ_{l=1}^{L} (0.5Â·I + 0.5Â·A^{(l)})</td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 px-3">Gradient Ã— Input</td>
                        <td class="py-2 px-3 font-mono text-xs">attr_i = |âˆ‚â„’/âˆ‚x_i Â· x_i|</td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 px-3">Integrated Gradients</td>
                        <td class="py-2 px-3 font-mono text-xs">IG_i = x_i Â· âˆ«_{Î±=0}^{1} âˆ‚F/âˆ‚x_i dÎ±</td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 px-3">SHAP Values</td>
                        <td class="py-2 px-3 font-mono text-xs">Ï†_i = âˆ‘_S |S|!(n-|S|-1)!/n! Â· Î”_i</td>
                    </tr>
                    <tr>
                        <td class="py-2 px-3">Leave-One-Out</td>
                        <td class="py-2 px-3 font-mono text-xs">Î”_p = P(y|all) - P(y|all\{p})</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div class="bg-gray-900 text-green-400 p-6 rounded-lg mb-8">
        <h3 class="font-semibold mb-4 text-white">Example: Attention Distribution for Obergefell v. Hodges (2015)</h3>
        <pre class="text-sm">
Query Case: Obergefell v. Hodges - Same-sex marriage constitutional right

<span class="text-yellow-400">Precedent Attention Weights (normalized):</span>
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
<span class="text-cyan-400">Loving v. Virginia (1967)</span>        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ <span class="text-white">0.31</span>
<span class="text-cyan-400">United States v. Windsor (2013)</span>  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   <span class="text-white">0.28</span>
<span class="text-cyan-400">Lawrence v. Texas (2003)</span>         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           <span class="text-white">0.21</span>
<span class="text-cyan-400">Romer v. Evans (1996)</span>            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    <span class="text-white">0.12</span>
<span class="text-cyan-400">Griswold v. Connecticut (1965)</span>   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         <span class="text-white">0.08</span>
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

<span class="text-yellow-400">Key Attended Phrases:</span>
â€¢ "fundamental right to marry" (Loving)           â†’ <span class="text-green-400">0.42</span> local attention
â€¢ "equal dignity in the eyes of the law" (Windsor) â†’ <span class="text-green-400">0.38</span> local attention
â€¢ "liberty protects the person" (Lawrence)         â†’ <span class="text-green-400">0.35</span> local attention

<span class="text-yellow-400">Model Prediction:</span> PETITIONER (confidence: 0.87)
<span class="text-yellow-400">Actual Outcome:</span>   PETITIONER âœ“
        </pre>
    </div>

    <div class="grid md:grid-cols-2 gap-8">
        <div>
            <h3 class="font-semibold mb-4">Observed Attention Patterns</h3>
            <table class="w-full text-sm">
                <thead>
                    <tr class="border-b bg-gray-50">
                        <th class="py-2 px-3 text-left">Pattern</th>
                        <th class="py-2 px-3 text-center">Effect</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="border-b"><td class="py-2 px-3">Recency bias</td><td class="py-2 px-3 text-center">1.4Ã— for recent</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Outcome alignment</td><td class="py-2 px-3 text-center">1.8Ã— for matching</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Citation distance</td><td class="py-2 px-3 text-center">2.1Ã— for direct</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">Issue area overlap</td><td class="py-2 px-3 text-center">1.6Ã— for same</td></tr>
                    <tr><td class="py-2 px-3">Relevance score</td><td class="py-2 px-3 text-center">r = 0.72 correlation</td></tr>
                </tbody>
            </table>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Faithfulness Evaluation</h3>
            <table class="w-full text-sm">
                <thead>
                    <tr class="border-b bg-gray-50">
                        <th class="py-2 px-3 text-left">Metric</th>
                        <th class="py-2 px-3 text-center">Value</th>
                        <th class="py-2 px-3 text-left">Interpretation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="border-b"><td class="py-2 px-3">Attention-Prediction r</td><td class="py-2 px-3 text-center font-mono">0.67</td><td class="py-2 px-3 text-gray-500">Strong</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">LOO Consistency</td><td class="py-2 px-3 text-center font-mono">83%</td><td class="py-2 px-3 text-gray-500">High</td></tr>
                    <tr class="border-b"><td class="py-2 px-3">ROAR@10%</td><td class="py-2 px-3 text-center font-mono">-12% acc</td><td class="py-2 px-3 text-gray-500">Meaningful</td></tr>
                    <tr><td class="py-2 px-3">Human Agreement Îº</td><td class="py-2 px-3 text-center font-mono">0.54</td><td class="py-2 px-3 text-gray-500">Moderate</td></tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<!-- Section 11: Calibration -->
<section id="calibration" class="card card-academic p-8 mb-8">
    <h2 class="section-header text-2xl font-bold">
        <span class="section-number">11</span>
        Model Calibration
    </h2>

    <p class="text-gray-600 mb-6">
        Well-calibrated probability estimates are essential for legal applications where practitioners need to assess prediction reliability. We apply temperature scaling (Guo et al., 2017) for post-hoc calibration.
    </p>

    <div class="grid md:grid-cols-2 gap-8 mb-8">
        <div>
            <h3 class="font-semibold mb-4">Expected Calibration Error (ECE)</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm mb-4">
                <pre>
ECE (Naeini et al., 2015):

ECE = âˆ‘_{m=1}^{M} (|B_m|/n) Â· |acc(B_m) - conf(B_m)|

Where:
  M = 15 equal-width bins
  B_m = samples in confidence bin m
  acc(B_m) = accuracy in bin m
  conf(B_m) = mean confidence in bin m

Interpretation:
  ECE = 0.00: perfect calibration
  ECE < 0.05: well-calibrated
  ECE > 0.15: poorly calibrated
                </pre>
            </div>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Temperature Scaling</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm mb-4">
                <pre>
Post-hoc calibration (Guo et al., 2017):

p_calibrated = softmax(z / T)

Where:
  z = logits from model
  T = temperature parameter

Optimization:
  T* = argmin_T NLL(y, softmax(z/T))

Effects:
  T > 1: soften predictions (less confident)
  T < 1: sharpen predictions (more confident)
  T = 1: no change
                </pre>
            </div>
        </div>
    </div>

    <div class="bg-blue-50 p-6 rounded-lg mb-8">
        <h3 class="font-semibold mb-4">Reliability Diagram</h3>
        <div class="grid md:grid-cols-2 gap-6">
            <div class="font-mono text-sm bg-white p-4 rounded overflow-x-auto">
                <pre>
Accuracy vs Confidence (15 bins):

1.0 â”‚                              â•­â”€â”€ Perfect
    â”‚                        â—   â•±    calibration
0.8 â”‚                   â—  â—   â•±
    â”‚              â—  â—     â•±    <span class="text-blue-600">â— After T-scaling</span>
    â”‚         â—‹  â—       â•±       <span class="text-gray-400">â—‹ Before</span>
0.6 â”‚      â—‹  â—  â—‹     â•±
    â”‚   â—‹  â—  â—‹     â•±
0.4 â”‚â—‹  â—        â•±
    â”‚          â•±
0.2 â”‚        â•±
    â”‚      â•±
0.0 â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
   0.0   0.25   0.5   0.75   1.0
              Confidence
                </pre>
            </div>
            <div>
                <h4 class="font-medium mb-3">Calibration Metrics</h4>
                <table class="w-full text-sm">
                    <tr class="border-b"><td class="py-2">ECE (before)</td><td class="py-2 font-mono text-red-600">0.127</td></tr>
                    <tr class="border-b"><td class="py-2">ECE (after T-scaling)</td><td class="py-2 font-mono text-green-600">0.034</td></tr>
                    <tr class="border-b"><td class="py-2">Optimal Temperature</td><td class="py-2 font-mono">T* = 1.42</td></tr>
                    <tr class="border-b"><td class="py-2">MCE (Max Cal. Error)</td><td class="py-2 font-mono">0.089</td></tr>
                    <tr class="border-b"><td class="py-2">Brier Score</td><td class="py-2 font-mono">0.168</td></tr>
                    <tr><td class="py-2">NLL (calibrated)</td><td class="py-2 font-mono">0.412</td></tr>
                </table>
            </div>
        </div>
    </div>

    <div class="grid md:grid-cols-2 gap-8">
        <div>
            <h3 class="font-semibold mb-4">Confidence Distribution</h3>
            <div class="space-y-2">
                <div class="flex items-center">
                    <span class="w-20 text-sm text-gray-600">0.5-0.6:</span>
                    <div class="flex-1 bg-gray-200 rounded h-6">
                        <div class="bg-blue-500 h-6 rounded flex items-center justify-end pr-2" style="width: 15%">
                            <span class="text-xs text-white">15%</span>
                        </div>
                    </div>
                </div>
                <div class="flex items-center">
                    <span class="w-20 text-sm text-gray-600">0.6-0.7:</span>
                    <div class="flex-1 bg-gray-200 rounded h-6">
                        <div class="bg-blue-500 h-6 rounded flex items-center justify-end pr-2" style="width: 22%">
                            <span class="text-xs text-white">22%</span>
                        </div>
                    </div>
                </div>
                <div class="flex items-center">
                    <span class="w-20 text-sm text-gray-600">0.7-0.8:</span>
                    <div class="flex-1 bg-gray-200 rounded h-6">
                        <div class="bg-blue-500 h-6 rounded flex items-center justify-end pr-2" style="width: 31%">
                            <span class="text-xs text-white">31%</span>
                        </div>
                    </div>
                </div>
                <div class="flex items-center">
                    <span class="w-20 text-sm text-gray-600">0.8-0.9:</span>
                    <div class="flex-1 bg-gray-200 rounded h-6">
                        <div class="bg-blue-500 h-6 rounded flex items-center justify-end pr-2" style="width: 24%">
                            <span class="text-xs text-white">24%</span>
                        </div>
                    </div>
                </div>
                <div class="flex items-center">
                    <span class="w-20 text-sm text-gray-600">0.9-1.0:</span>
                    <div class="flex-1 bg-gray-200 rounded h-6">
                        <div class="bg-blue-500 h-6 rounded flex items-center justify-end pr-2" style="width: 8%">
                            <span class="text-xs text-white">8%</span>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Practical Implications</h3>
            <div class="callout callout-success">
                <h4 class="font-medium mb-2">Well-Calibrated for Legal Use</h4>
                <p class="text-sm text-gray-600">
                    After temperature scaling, ECE of 0.034 means when the model predicts 70% confidence, it's correct approximately 70% of the time. This reliability is crucial for:
                </p>
                <ul class="text-sm text-gray-600 mt-2 space-y-1">
                    <li>â€¢ Risk assessment by legal practitioners</li>
                    <li>â€¢ Identifying cases needing human review</li>
                    <li>â€¢ Setting decision thresholds for different use cases</li>
                </ul>
            </div>
        </div>
    </div>
</section>

<!-- Section 12: Statistical Tests -->
<section id="statistical-tests" class="card card-academic p-8 mb-8">
    <h2 class="section-header text-2xl font-bold">
        <span class="section-number">12</span>
        Statistical Significance Testing
    </h2>

    <p class="text-gray-600 mb-6">
        Rigorous statistical methods for comparing model performance and establishing significance of improvements over baselines.
    </p>

    <div class="grid md:grid-cols-2 gap-8 mb-8">
        <div>
            <h3 class="font-semibold mb-4">McNemar's Test</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm mb-4">
                <pre>
McNemar's Test (Dietterich, 1998):

Contingency table for paired predictions:

              Model B
            Correct  Wrong
Model A  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Correct  â”‚    a    â”‚    b    â”‚
Wrong    â”‚    c    â”‚    d    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Test statistic:
  Ï‡Â² = (|b - c| - 1)Â² / (b + c)

Hâ‚€: P(b) = P(c) (models equivalent)
Hâ‚: P(b) â‰  P(c) (models differ)

Reject Hâ‚€ if Ï‡Â² > 3.84 (Î± = 0.05)
                </pre>
            </div>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Bootstrap Confidence Intervals</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm mb-4">
                <pre>
BCa Bootstrap (Efron & Tibshirani, 1993):

for b in 1..B:  # B = 10,000
    D_b = sample(D_test, n, replace=True)
    Î¸_b = metric(model, D_b)

Percentile CI:
  CI_95% = [Î¸_{(0.025Â·B)}, Î¸_{(0.975Â·B)}]

BCa Correction:
  zâ‚€ = Î¦â»Â¹(#{Î¸_b < Î¸Ì‚} / B)
  a = âˆ‘(Î¸Ì„ - Î¸_i)Â³ / (6Â·(âˆ‘(Î¸Ì„ - Î¸_i)Â²)^1.5)

Adjusted percentiles account for bias
and skewness in bootstrap distribution.
                </pre>
            </div>
        </div>
    </div>

    <div class="bg-green-50 p-6 rounded-lg mb-8">
        <h3 class="font-semibold mb-4">Statistical Comparison Results</h3>
        <div class="overflow-x-auto">
            <table class="w-full text-sm">
                <thead>
                    <tr class="border-b bg-green-100">
                        <th class="py-3 px-4 text-left">Comparison</th>
                        <th class="py-3 px-4 text-center">AUROC Î”</th>
                        <th class="py-3 px-4 text-center">95% CI</th>
                        <th class="py-3 px-4 text-center">McNemar Ï‡Â²</th>
                        <th class="py-3 px-4 text-center">p-value</th>
                        <th class="py-3 px-4 text-center">Significant?</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="border-b">
                        <td class="py-3 px-4">Ours vs. Mistral (no retrieval)</td>
                        <td class="py-3 px-4 text-center font-mono">+0.06</td>
                        <td class="py-3 px-4 text-center font-mono">[0.04, 0.08]</td>
                        <td class="py-3 px-4 text-center font-mono">18.7</td>
                        <td class="py-3 px-4 text-center font-mono">&lt;0.001</td>
                        <td class="py-3 px-4 text-center text-green-600">Yes ***</td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-3 px-4">Ours vs. BM25 Retrieval</td>
                        <td class="py-3 px-4 text-center font-mono">+0.03</td>
                        <td class="py-3 px-4 text-center font-mono">[0.01, 0.05]</td>
                        <td class="py-3 px-4 text-center font-mono">8.4</td>
                        <td class="py-3 px-4 text-center font-mono">0.004</td>
                        <td class="py-3 px-4 text-center text-green-600">Yes **</td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-3 px-4">Ours vs. Longformer</td>
                        <td class="py-3 px-4 text-center font-mono">+0.07</td>
                        <td class="py-3 px-4 text-center font-mono">[0.05, 0.09]</td>
                        <td class="py-3 px-4 text-center font-mono">24.1</td>
                        <td class="py-3 px-4 text-center font-mono">&lt;0.001</td>
                        <td class="py-3 px-4 text-center text-green-600">Yes ***</td>
                    </tr>
                    <tr>
                        <td class="py-3 px-4">Ours vs. Legal-BERT</td>
                        <td class="py-3 px-4 text-center font-mono">+0.09</td>
                        <td class="py-3 px-4 text-center font-mono">[0.06, 0.12]</td>
                        <td class="py-3 px-4 text-center font-mono">31.5</td>
                        <td class="py-3 px-4 text-center font-mono">&lt;0.001</td>
                        <td class="py-3 px-4 text-center text-green-600">Yes ***</td>
                    </tr>
                </tbody>
            </table>
        </div>
        <p class="text-xs text-gray-500 mt-3">
            *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 (Bonferroni-corrected for 4 comparisons, Î± = 0.0125)
        </p>
    </div>

    <div class="grid md:grid-cols-3 gap-6">
        <div>
            <h3 class="font-semibold mb-4">Effect Size (Cohen's h)</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-sm">
                <pre>
Cohen's h for proportions:

h = 2Â·arcsin(âˆšpâ‚) - 2Â·arcsin(âˆšpâ‚‚)

Interpretation:
  |h| < 0.2: small effect
  |h| â‰ˆ 0.5: medium effect
  |h| > 0.8: large effect

Our improvement vs baseline:
  h = 0.62 (medium-large)
                </pre>
            </div>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Multiple Testing Correction</h3>
            <table class="w-full text-sm">
                <tr class="border-b"><td class="py-2 text-gray-600">Method</td><td class="py-2 font-mono">Bonferroni</td></tr>
                <tr class="border-b"><td class="py-2 text-gray-600"># Comparisons</td><td class="py-2 font-mono">4</td></tr>
                <tr class="border-b"><td class="py-2 text-gray-600">Î± (original)</td><td class="py-2 font-mono">0.05</td></tr>
                <tr class="border-b"><td class="py-2 text-gray-600">Î± (corrected)</td><td class="py-2 font-mono">0.0125</td></tr>
                <tr><td class="py-2 text-gray-600">All significant?</td><td class="py-2 font-mono text-green-600">Yes âœ“</td></tr>
            </table>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Variance Analysis</h3>
            <table class="w-full text-sm">
                <tr class="border-b"><td class="py-2 text-gray-600">Cross-val folds</td><td class="py-2 font-mono">5</td></tr>
                <tr class="border-b"><td class="py-2 text-gray-600">AUROC mean</td><td class="py-2 font-mono">0.798</td></tr>
                <tr class="border-b"><td class="py-2 text-gray-600">AUROC std</td><td class="py-2 font-mono">Â±0.012</td></tr>
                <tr class="border-b"><td class="py-2 text-gray-600">Seeds tested</td><td class="py-2 font-mono">3</td></tr>
                <tr><td class="py-2 text-gray-600">Seed variance</td><td class="py-2 font-mono">Â±0.008</td></tr>
            </table>
        </div>
    </div>
</section>

<!-- Section 13: Computational Requirements -->
<section id="computational" class="card card-academic p-8 mb-8">
    <h2 class="section-header text-2xl font-bold">
        <span class="section-number">13</span>
        Computational Requirements
    </h2>

    <div class="grid md:grid-cols-3 gap-6 mb-8">
        <div class="metric-card bg-gradient-to-br from-green-50 to-white border-2 border-green-200">
            <div class="metric-value text-green-600">~4h</div>
            <div class="metric-label">Training Time</div>
            <div class="text-xs text-gray-500 mt-1">Single A100 GPU</div>
        </div>
        <div class="metric-card bg-gradient-to-br from-blue-50 to-white border-2 border-blue-200">
            <div class="metric-value text-blue-600">3s</div>
            <div class="metric-label">Inference/Case</div>
            <div class="text-xs text-gray-500 mt-1">~20 cases/minute</div>
        </div>
        <div class="metric-card bg-gradient-to-br from-purple-50 to-white border-2 border-purple-200">
            <div class="metric-value text-purple-600">$30</div>
            <div class="metric-label">Total Cost</div>
            <div class="text-xs text-gray-500 mt-1">Complete pipeline</div>
        </div>
    </div>

    <div class="overflow-x-auto mb-8">
        <table class="w-full text-sm">
            <thead>
                <tr class="border-b bg-gray-50">
                    <th class="py-3 px-4 text-left">Component</th>
                    <th class="py-3 px-4 text-center">Time</th>
                    <th class="py-3 px-4 text-center">Hardware</th>
                    <th class="py-3 px-4 text-center">Memory</th>
                    <th class="py-3 px-4 text-center">Est. Cost</th>
                </tr>
            </thead>
            <tbody>
                <tr class="border-b">
                    <td class="py-3 px-4">Data preprocessing</td>
                    <td class="py-3 px-4 text-center font-mono">~30 min</td>
                    <td class="py-3 px-4 text-center">CPU (8 cores)</td>
                    <td class="py-3 px-4 text-center font-mono">8GB RAM</td>
                    <td class="py-3 px-4 text-center">$0</td>
                </tr>
                <tr class="border-b">
                    <td class="py-3 px-4">Citation extraction</td>
                    <td class="py-3 px-4 text-center font-mono">~10 min</td>
                    <td class="py-3 px-4 text-center">CPU</td>
                    <td class="py-3 px-4 text-center font-mono">4GB RAM</td>
                    <td class="py-3 px-4 text-center">$0</td>
                </tr>
                <tr class="border-b">
                    <td class="py-3 px-4">GraphSAGE training</td>
                    <td class="py-3 px-4 text-center font-mono">~30 min</td>
                    <td class="py-3 px-4 text-center">T4 GPU</td>
                    <td class="py-3 px-4 text-center font-mono">12GB VRAM</td>
                    <td class="py-3 px-4 text-center">~$1</td>
                </tr>
                <tr class="border-b">
                    <td class="py-3 px-4">QLoRA fine-tuning</td>
                    <td class="py-3 px-4 text-center font-mono">~4 hours</td>
                    <td class="py-3 px-4 text-center">A100 80GB</td>
                    <td class="py-3 px-4 text-center font-mono">35GB VRAM</td>
                    <td class="py-3 px-4 text-center">~$15</td>
                </tr>
                <tr class="border-b">
                    <td class="py-3 px-4">Evaluation</td>
                    <td class="py-3 px-4 text-center font-mono">~15 min</td>
                    <td class="py-3 px-4 text-center">A100</td>
                    <td class="py-3 px-4 text-center font-mono">20GB VRAM</td>
                    <td class="py-3 px-4 text-center">~$1</td>
                </tr>
                <tr class="bg-gray-50">
                    <td class="py-3 px-4 font-bold">Total</td>
                    <td class="py-3 px-4 text-center font-mono font-bold">~5.5 hours</td>
                    <td class="py-3 px-4 text-center">â€”</td>
                    <td class="py-3 px-4 text-center">â€”</td>
                    <td class="py-3 px-4 text-center font-bold">~$17-30</td>
                </tr>
            </tbody>
        </table>
    </div>

    <div class="grid md:grid-cols-2 gap-6">
        <div class="callout callout-info">
            <h4 class="font-medium mb-2">Cloud Platform</h4>
            <p class="text-sm text-gray-600">
                All experiments run on <strong>Modal Labs</strong> with A100 GPUs at ~$3.50/hour. Code designed for serverless execution with automatic scaling.
            </p>
        </div>
        <div class="callout callout-success">
            <h4 class="font-medium mb-2">Reproducibility Cost</h4>
            <p class="text-sm text-gray-600">
                Total compute cost under $30 makes this research accessible to academic labs and independent researchers without institutional GPU clusters.
            </p>
        </div>
    </div>
</section>

<!-- Section 14: Reproducibility -->
<section id="reproducibility" class="card card-academic p-8">
    <h2 class="section-header text-2xl font-bold">
        <span class="section-number">14</span>
        Reproducibility Checklist
    </h2>

    <p class="text-gray-600 mb-6">
        Following EMNLP reproducibility guidelines, we provide comprehensive documentation for result replication.
    </p>

    <div class="grid md:grid-cols-2 gap-8 mb-8">
        <div>
            <h3 class="font-semibold mb-4">Code & Data Availability</h3>
            <table class="w-full text-sm">
                <tbody>
                    <tr class="border-b">
                        <td class="py-2 text-gray-600">Source Code</td>
                        <td class="py-2"><a href="#" class="text-blue-600 hover:underline">github.com/[repo]</a></td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 text-gray-600">License</td>
                        <td class="py-2 font-mono">MIT</td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 text-gray-600">SCDB Data</td>
                        <td class="py-2"><a href="http://scdb.wustl.edu" class="text-blue-600 hover:underline">scdb.wustl.edu</a></td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 text-gray-600">Case Text API</td>
                        <td class="py-2"><a href="https://www.courtlistener.com" class="text-blue-600 hover:underline">courtlistener.com</a></td>
                    </tr>
                    <tr class="border-b">
                        <td class="py-2 text-gray-600">Trained Models</td>
                        <td class="py-2">HuggingFace Hub</td>
                    </tr>
                    <tr>
                        <td class="py-2 text-gray-600">Processed Data</td>
                        <td class="py-2">Zenodo archive</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Environment Specification</h3>
            <div class="bg-gray-100 p-4 rounded-lg font-mono text-xs">
                <pre>
# Key dependencies (requirements.txt)
torch==2.1.0
transformers==4.36.0
peft==0.7.0
bitsandbytes==0.41.3
torch-geometric==2.4.0
sentence-transformers==2.2.2
neo4j==5.14.0
scikit-learn==1.3.2
modal==0.56.0

# Python version
python==3.10.12

# CUDA version
cuda==12.1
                </pre>
            </div>
        </div>
    </div>

    <div class="bg-yellow-50 p-6 rounded-lg mb-6">
        <h3 class="font-semibold mb-4">Random Seeds & Determinism</h3>
        <div class="grid md:grid-cols-3 gap-4">
            <div class="bg-white p-3 rounded">
                <div class="font-mono text-sm">SEED = 42</div>
                <div class="text-xs text-gray-500">All experiments</div>
            </div>
            <div class="bg-white p-3 rounded">
                <div class="font-mono text-sm">torch.backends.cudnn.deterministic = True</div>
                <div class="text-xs text-gray-500">CUDA determinism</div>
            </div>
            <div class="bg-white p-3 rounded">
                <div class="font-mono text-sm">Data splits saved</div>
                <div class="text-xs text-gray-500">data/splits/*.json</div>
            </div>
        </div>
    </div>

    <div class="grid md:grid-cols-2 gap-6">
        <div>
            <h3 class="font-semibold mb-4">EMNLP Checklist Items</h3>
            <ul class="space-y-2 text-sm">
                <li class="flex items-center"><span class="text-green-500 mr-2">âœ“</span> Hyperparameters documented</li>
                <li class="flex items-center"><span class="text-green-500 mr-2">âœ“</span> Training/evaluation code provided</li>
                <li class="flex items-center"><span class="text-green-500 mr-2">âœ“</span> Data preprocessing scripts included</li>
                <li class="flex items-center"><span class="text-green-500 mr-2">âœ“</span> Model checkpoints available</li>
                <li class="flex items-center"><span class="text-green-500 mr-2">âœ“</span> Statistical significance tests</li>
                <li class="flex items-center"><span class="text-green-500 mr-2">âœ“</span> Compute requirements stated</li>
                <li class="flex items-center"><span class="text-green-500 mr-2">âœ“</span> Variance across runs reported</li>
                <li class="flex items-center"><span class="text-green-500 mr-2">âœ“</span> License specified</li>
            </ul>
        </div>

        <div>
            <h3 class="font-semibold mb-4">Running Experiments</h3>
            <div class="bg-gray-900 text-green-400 p-4 rounded-lg font-mono text-xs">
                <pre>
<span class="text-gray-500"># Clone repository</span>
git clone https://github.com/[repo]
cd caselaw-graph-ring

<span class="text-gray-500"># Install dependencies</span>
pip install -r requirements.txt

<span class="text-gray-500"># Download data</span>
python scripts/download_data.py

<span class="text-gray-500"># Train GraphSAGE</span>
python -m src.graph.train

<span class="text-gray-500"># Fine-tune LLM (Modal)</span>
modal run src/model/train.py

<span class="text-gray-500"># Evaluate</span>
python -m src.model.evaluate
                </pre>
            </div>
        </div>
    </div>
</section>
{% endblock %}
